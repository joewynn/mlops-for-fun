{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Random Forest Classifier on Wine Dataset using `scikit-learn`\n",
    "\n",
    "- Compute the feature importance score by permutating each feature\n",
    "- Re-train the model with only the top features\n",
    "- check outher classifiers for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and pre-process the data\n",
    "\n",
    "Import the required dependencies and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "import pickle\n",
    "\n",
    "# as_frame param requires `scikit-learn` >= 0.23\n",
    "data = load_wine(as_frame=True)\n",
    "\n",
    "# Pring first rows of the data\n",
    "data.frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is made up of 13 numerical features  and there are three different classes of wine\n",
    "\n",
    "Perform train/test split and normalize the data with `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train /Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
    "\n",
    "# Instatiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit it to the train data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Use it to transform the train and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Notice that the scaler is trained on the train data to avoid data leakage from the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Classifier\n",
    "\n",
    "Fit a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with 10 estimators and copute the mean accuracy achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit the classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=10, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Print the mean accuracy the classifier achieves on the test set\n",
    "rf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model achieved a mean accuracy of 91%. Pretty good for a model without fine tunning. \n",
    "\n",
    "## Permutation Feature Importance\n",
    "\n",
    "To perform the model inspection technique known as Permutation Feature Importance you will use `scikit-learn`'s built-in [permutation_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance). \n",
    "\n",
    "Create a function that given a classifier, features and lables computes the feature importance for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def feature_importance(clf, X, y, top_limit=None):\n",
    "\n",
    "    # Retrieve the Bunch object after 50 repeats\n",
    "    # n_repeats is the number of times that each feature was permutated to the computer; the final score\n",
    "    bunch = permutation_importance(clf, X, y, \n",
    "                                   n_repeats=50, random_state=42)\n",
    "    # Average feature importance\n",
    "    imp_means = bunch.importances_mean\n",
    "    \n",
    "    # List that contains the index of each feature in descending order of importance\n",
    "    ordered_imp_means_args = np.argsort(imp_means)[::-1]\n",
    "\n",
    "    # If no limit, print all features\n",
    "    if top_limit is None:\n",
    "        top_limit = len(ordered_imp_means_args)\n",
    "\n",
    "    # Print relevant information \n",
    "    for i, _ in zip(ordered_imp_means_args, range(top_limit)):\n",
    "        name = data.feature_names[i]\n",
    "        imp_score = imp_means[i]\n",
    "        imp_std = bunch.importances_std[i]\n",
    "        print(f\"Feature {name} with index {i} has an average importance score of {imp_score:.3f} +/- {imp_std:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance score is computed in a way that higher values represent better predictive power. \n",
    "\n",
    "Use the `feature_importance` function on the `Random Forest Classifier` and the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature flavanoids with index 6 has an average importance score of 0.227 +/- 0.025\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.142 +/- 0.019\n",
      "\n",
      "Feature color_intensity with index 9 has an average importance score of 0.112 +/- 0.023\n",
      "\n",
      "Feature od280/od315_of_diluted_wines with index 11 has an average importance score of 0.007 +/- 0.005\n",
      "\n",
      "Feature total_phenols with index 5 has an average importance score of 0.003 +/- 0.004\n",
      "\n",
      "Feature malic_acid with index 1 has an average importance score of 0.002 +/- 0.004\n",
      "\n",
      "Feature proanthocyanins with index 8 has an average importance score of 0.002 +/- 0.003\n",
      "\n",
      "Feature hue with index 10 has an average importance score of 0.002 +/- 0.003\n",
      "\n",
      "Feature nonflavanoid_phenols with index 7 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature magnesium with index 4 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature alcalinity_of_ash with index 3 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature ash with index 2 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature alcohol with index 0 has an average importance score of 0.000 +/- 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_importance(rf_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many of the features have a fairly low importance score. This points that the predictive power of this dataset is condensed in a few features. \n",
    "\n",
    "However, it is impotant to notice that this process was done for the training set, so this feature importance does NOT have into account if the feature might help with the genrralization power of the model. \n",
    "\n",
    "to check this, we repeate the process for the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature flavanoids with index 6 has an average importance score of 0.202 +/- 0.047\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.143 +/- 0.042\n",
      "\n",
      "Feature color_intensity with index 9 has an average importance score of 0.112 +/- 0.043\n",
      "\n",
      "Feature alcohol with index 0 has an average importance score of 0.024 +/- 0.017\n",
      "\n",
      "Feature magnesium with index 4 has an average importance score of 0.021 +/- 0.015\n",
      "\n",
      "Feature od280/od315_of_diluted_wines with index 11 has an average importance score of 0.015 +/- 0.018\n",
      "\n",
      "Feature hue with index 10 has an average importance score of 0.013 +/- 0.018\n",
      "\n",
      "Feature total_phenols with index 5 has an average importance score of 0.002 +/- 0.016\n",
      "\n",
      "Feature nonflavanoid_phenols with index 7 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature alcalinity_of_ash with index 3 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature malic_acid with index 1 has an average importance score of -0.002 +/- 0.017\n",
      "\n",
      "Feature ash with index 2 has an average importance score of -0.003 +/- 0.008\n",
      "\n",
      "Feature proanthocyanins with index 8 has an average importance score of -0.021 +/- 0.020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_importance(rf_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the top most important features are the same for both sets. However, features such as *alcohol*, considered unimportant for the training set, are much more critical when using the testing set. This hints that this feature will contribute to the generalization power of the model.\n",
    "\n",
    "If a feature is deemed important for the train set but not for the testing, this feature will probably cause the model to overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train the model with the most important features\n",
    "\n",
    "Re-train the `Random Forest Classifier` with only the top 3 most important features. \n",
    "\n",
    "In this case they are the same for both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On TRAIN split:\n",
      "\n",
      "Feature flavanoids with index 6 has an average importance score of 0.227 +/- 0.025\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.142 +/- 0.019\n",
      "\n",
      "Feature color_intensity with index 9 has an average importance score of 0.112 +/- 0.023\n",
      "\n",
      "\n",
      "On TEST split:\n",
      "\n",
      "Feature flavanoids with index 6 has an average importance score of 0.202 +/- 0.047\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.143 +/- 0.042\n",
      "\n",
      "Feature color_intensity with index 9 has an average importance score of 0.112 +/- 0.043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"On TRAIN split:\\n\")\n",
    "feature_importance(rf_clf, X_train, y_train, top_limit=3)\n",
    "\n",
    "print(\"\\nOn TEST split:\\n\")\n",
    "feature_importance(rf_clf, X_test, y_test, top_limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preserve only the top 3 features\n",
    "X_train_top_features = X_train[:,[6, 9, 12]]\n",
    "X_test_top_features = X_test[:,[6, 9, 12]]\n",
    "\n",
    "# Re-train with only these features\n",
    "rf_clf_top = RandomForestClassifier(n_estimators=10, random_state=42).fit(X_train_top_features, y_train)\n",
    "\n",
    "# Compute mean accuracy achieved\n",
    "rf_clf_top.score(X_test_top_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by using only the 3 most important features, the model achieved a mean accuracy even higher than the one using all 13 features. \n",
    "\n",
    "Remember that *alcohol* feature was deemed not imporatn in the train split but you had the hypotheses that it had important information for the generalization of the model. \n",
    "\n",
    "Let's add this feature and see how the model performas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preserve only the top 3 features and alcohol\n",
    "X_train_top_features = X_train[:,[0, 6, 9, 12]]\n",
    "X_test_top_features = X_test[:,[0, 6, 9, 12]]\n",
    "\n",
    "# Re-train with only these features\n",
    "rf_clf_top = RandomForestClassifier(n_estimators=10, random_state=42).fit(X_train_top_features, y_train)\n",
    "\n",
    "# Compute mean accuracy achieved\n",
    "rf_clf_top.score(X_test_top_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Adding this additional feature, we now get a mean accuracy of 100%. \n",
    "It looks like this feature did provide some important information that helped the model do a better job at generalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Other Classifiers\n",
    "\n",
    "The process of Permutation Feature Importance is also dependent on the classifier you are using. Since different classifiers follow different rules, it is natural to assume they consider different features important or unimportant. \n",
    "\n",
    "So, let's try other classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "=> Lasso classifier \n",
      "\n",
      "Mean accuracy on the test set: 29.48%\n",
      "\n",
      "Top 4 features when using the test set:\n",
      "\n",
      "Feature flavanoids with index 6 has an average importance score of 0.361 +/- 0.061\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature od280/od315_of_diluted_wines with index 11 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "Feature hue with index 10 has an average importance score of 0.000 +/- 0.000\n",
      "\n",
      "====================================================================================================\n",
      "=> Ridge classifier \n",
      "\n",
      "Mean accuracy on the test set: 88.71%\n",
      "\n",
      "Top 4 features when using the test set:\n",
      "\n",
      "Feature flavanoids with index 6 has an average importance score of 0.445 +/- 0.071\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.210 +/- 0.035\n",
      "\n",
      "Feature color_intensity with index 9 has an average importance score of 0.119 +/- 0.029\n",
      "\n",
      "Feature od280/od315_of_diluted_wines with index 11 has an average importance score of 0.111 +/- 0.026\n",
      "\n",
      "====================================================================================================\n",
      "=> Decision Tree classifier \n",
      "\n",
      "Mean accuracy on the test set: 95.56%\n",
      "\n",
      "Top 4 features when using the test set:\n",
      "\n",
      "Feature flavanoids with index 6 has an average importance score of 0.297 +/- 0.061\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.143 +/- 0.039\n",
      "\n",
      "Feature color_intensity with index 9 has an average importance score of 0.131 +/- 0.037\n",
      "\n",
      "Feature alcohol with index 0 has an average importance score of 0.049 +/- 0.020\n",
      "\n",
      "====================================================================================================\n",
      "=> Support Vector classifier \n",
      "\n",
      "Mean accuracy on the test set: 97.78%\n",
      "\n",
      "Top 4 features when using the test set:\n",
      "\n",
      "Feature proline with index 12 has an average importance score of 0.069 +/- 0.031\n",
      "\n",
      "Feature flavanoids with index 6 has an average importance score of 0.061 +/- 0.023\n",
      "\n",
      "Feature alcohol with index 0 has an average importance score of 0.044 +/- 0.023\n",
      "\n",
      "Feature ash with index 2 has an average importance score of 0.032 +/- 0.018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Select 4 new classifiers\n",
    "clfs = {\"Lasso\": Lasso(alpha=0.5), \n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Support Vector\": SVC()}\n",
    "\n",
    "# Compute feature importance on test set given a classifier\n",
    "\n",
    "def fit_compute_importance(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Mean accuracy on the test set: {clf.score(X_test, y_test)*100:.2f}%\\n\")\n",
    "    print(f\"Top 4 features when using the test set:\\n\")\n",
    "    feature_importance(clf, X_test, y_test, top_limit=4)\n",
    "\n",
    "# Print results\n",
    "for name, clf in clfs.items():\n",
    "    print(\"=====\"*20)\n",
    "    print(f\"=> {name} classifier \\n\")\n",
    "    fit_compute_importance(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like _flavanoids_ and _proline_ are very important accross all classifiers. However, there is variability from one classifier to the others on what features are considered the most important ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the model for portability\n",
    "\n",
    "Serialize the `rf_clf_top` model with `piclkle` to make it portable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_clf_top, open('wine_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model from Pickle\n",
    "\n",
    "To load a saved model from a `Pickle` file, all you need to do is pass the `pickled` model into the `Pickle` `load()` function, and it will be deserialized. \n",
    "\n",
    "By assigning this back to a model object, you can run your original model’s `predict()` function, pass in some test data and get back an array of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 1,\n",
       "       2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model = pickle.load(open('wine_model.pkl', 'rb'))\n",
    "pickled_model.predict(X_test_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('wine_dtc_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 1,\n",
       "       2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model = pickle.load(open('wine_dtc_model.pkl', 'rb'))\n",
    "pickled_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
